Niche介绍
Niche，意为小众，项目中文名为【小众盒】。小众盒是为了解决身边朋友提出的一个痛点而诞生的，我们想要一个专注熟人社交的社交APP，相当于一个内容种类和交互方式更为丰富的微信朋友圈。
你可以跟喜欢打游戏的朋友组建一个盒子，在里面分享游戏心得，也可以跟喜欢乐器的朋友组建一个盒子，在里面交流乐器的学习。圈子的组建是随心所欲的，你可以根据不同的兴趣爱好组建，也可以给高中同学建一个然后大学同学建另外一个。总而言之，当你发微信朋友圈时点开了”谁可以看“按钮，在思考这条朋友圈是给哪一部分人看的时候，说不定已经踩在了Niche的核心feature上。
Niche技术文档
一. 总体设计
1.架构设计

2.用例设计

二. 服务划分
Niche是一个基于go-zero开发的微服务项目，目前对其划分了五大模块共计16个服务，目前已经实现大部分服务，核心流程已经能形成闭环。


三. 服务设计
tips1：如无特殊说明，以下所有服务的数据库表都是使用自增id作为主键，业务id由雪花算法生成。
tips2：如无特殊说明，以下所有服务的数据库表都会包含gorm自带的Model（包含自增id、创建时间戳、更新时间戳、删除时间戳字段），数据库设计中不会再写出这些字段。
tips3：如无特殊说明，以下所有服务的缓存都是使用Cache Aside方案与数据库保持一致性。
1.用户模块
- 用户服务
主要功能：负责管理用户的基础信息，实现注册、登录、用户信息管理等接口。
数据库设计：

主要索引有 
① uniq_users_email，【email】字段，唯一索引，注册时通过唯一索引来防止重复注册。
② uniq_users_uid，【uid】字段，唯一索引，用户的业务id。
缓存设计：
Todo
核心业务逻辑：
① 登录功能
登录功能基于JWT实现双Token模式，登录成功后生成AccessToken和RefreshToken，AccessToken过期时间为1小时，RefreshToken过期时间为7x24小时。AccessToken过期后，首先向Refresh接口尝试刷新AccessToken，如果请求成功，则生成新的AccessToken和RefreshToken，如果失败则提示用户重新登录。
② 注册功能
注册功能时序图如下

验证码存放在Redis中，设置5分钟过期时间，过期、成功注册、重新发送验证码都会删除Redis的验证码。

- 关系服务
参考文献：【哔哩哔哩技术】百亿数据百万查询——关系链架构演进
主要功能：负责管理用户之间的关注、粉丝等关系，实现关注、取消关注、查看关注列表、查看粉丝列表等接口，同时给Feed服务推模式、拉模式的实现提供关系数据。
数据库设计：

主要索引有 
① idx_relationships_uid，【uid】字段，普通索引。
② idx_relationships_fid，【fid】字段，普通索引。
③ idx_relationship_counts_uid，【uid】字段，计数表的主键索引。
缓存设计：
① 用户的关注列表缓存

因为用户之间的关系有Follow（单向关注）、Friend（互相关注），所以使用Hash数据结构去缓存，hashvalue就是保存了是Follow关系还是Friend关系。
② 用户的关注、粉丝数量计数缓存

使用一张表保存关注、粉丝两个计数，String的Value中间使用 ”,“ 分隔，前半部分是粉丝数量，后半部分是关注数量。
核心业务逻辑：
① 查找用户A与用户B的关系
因为大部分用户之间是没有关系的，所以在缓存和数据库中都没有记录，这就是典型的【缓存穿透】问题。因此我引入了【布隆过滤器】来过滤大部分的请求，只让有可能存在关系的请求真正走到业务逻辑中。

如果用户A与B可能有关系，那么首先尝试从”用户的关注列表“缓存中获取数据，只要用户A和B的其中一个关注列表有对方的uid存在，就说明他们之间存在关系，并且能够判断出是”谁关注了谁“。
如果缓存失效，则向数据库中查找，因为我们给uid、fid两个字段都上了索引，所以查询效率也比较高。

2.盒子模块
- 盒子内容服务
主要功能：负责管理盒子内部内容的展示顺序。
缓存设计：
① 内容id的索引缓存

使用Redis的ZSet去保存某个盒子内的内容排序列表，不同的排序方式用不同的ZSet去保存，例如按照创建时间排序的score是create_time，按照点赞量排序的score是like_count。该表的Member是MessageID，也就是只保存内容的id，不保存具体数据，所以这个缓存是用作索引的，提升查询时的分页、排序效率。
核心业务逻辑：
① 索引缓存的更新方式
该缓存的数据来源于图文服务、视频服务，通过Cron定时任务，每10秒钟从图文服务、视频服务的接口中获取【新增】、【删除】的内容id，把这些内容id通过ZSet的zadd、zrem命令把数据更新到索引缓存里，由此来实现缓存的一致性。
Q：为什么不使用Cache Aside方案来更新缓存？
A：Cache Aside方案在更新数据库时会删除缓存，然后在下次读缓存miss时从数据库中读取并回写缓存。因为本缓存的更新频率较高，重建消耗大，所以不适合Cache Aside方案。
Q：为什么不选择直接更新缓存的方式？
A：Todo
- 盒子信息服务
Todo
- 盒子成员服务
Todo

3.内容模块
- 图文服务
主要功能：负责管理图文帖子，实现对图文的增删改查等接口。
数据库设计：

主要索引有
① uniq_posts_post_id，【post_id】字段，唯一索引，图文帖子的业务id。
② idx_posts_author_id，【author_id】字段，普通索引。
缓存设计：
Todo
核心业务逻辑：
① user view的统计方式
uv的统计是基于Redis的HyperLogLog实现的，HyperLogLog是专门为统计海量数据而设计的一种数据类型，它能使用很小的内存完成对海量数据的统计操作，它可能会有微小的误差。对于uv数据的统计场景，我们能够容忍一部分误差数据，所以HyperLogLog非常适合。
因为访问帖子是一个极其频繁的操作，我们不可能在每一次访问帖子时都尝试把Redis的uv数据更新到数据库中，这么做对数据库的压力很大。所以我采取了一种新的方案来实现uv数据的同步。每天当某个帖子被第一次访问时，通过Kafka异步地把post_id写入到一个专门的“任务表”中，Cron定时任务在每天凌晨两点半启动任务。定时任务的逻辑是从“任务表”中读出前一天中uv数据发生变化的post_id，然后根据post_id读取Redis的HyperLogLog数据，更新到数据库的userView字段中。
Q：为什么要采用这种方案？
A：这样能把写数据库的操作延迟到凌晨这个数据库压力最小的时段。
Q：任务表如何设计？
A：Todo
Q：一天的任务如何去重？
A：因为任务表记录的是“每天有哪些帖子的uv发生了变化”，所以即使一个帖子今天被访问了多次，我们也只需为它记录一次任务。去重的方式是引入布隆过滤器，每次向Kafka投递“添加任务”的消息前，首先检查今天的布隆过滤器中是否有这个post_id，如果有则说明任务表中已经存在这个任务，那就不需要投递了。

- 点赞服务
参考文献：【哔哩哔哩技术】B站千亿级点赞系统服务架构设计
主要功能：负责管理用户对各种内容（图文、评论等）的点赞记录、点赞计数等数据。
数据库设计：

主要索引有
① uniq_likes_uid_msgid_msgtype，【uid, messageID, messageType】字段，联合唯一索引。该索引用于防止重复点赞。
② idx_like_counts_msgid_msgtype，【messageID, messageType】字段，联合索引，计数表的主键索引。
缓存设计：
① 某个内容的点赞计数缓存

② 用户的点赞历史记录缓存

用户的点赞历史记录指的是记录用户最近对某个MessageType的最新20条点赞记录，这个ZSet容量是有上限的，每次ZAdd后都需要检查长度，如果长度超过20则舍弃后面的部分。如果需要查找20条后面的点赞记录，则需要回溯到数据库进行查找。
核心业务逻辑：
① 检查用户是否点赞过某个内容
以B站为例，当我们进入某个视频中，它会显示我们是否点赞过这个视频，如果点赞过，那么大拇指就会亮起来，如下图

那么，我们真的需要每次点开一个视频都去数据库中查找是否点赞过这个视频吗？不仅仅是视频，评论区的每一条评论都有点赞功能，如果需要给每一条评论都去数据库中查看是否点赞过，这个查询请求的重量也太大了。而且，用户对大部分的内容都是未点赞过的状态，也就是说缓存、数据库中不会有点赞记录，这也是经典的【缓存穿透】问题。跟前面关系服务的做法一样，我也引入了布隆过滤器来解决。
Q：布隆过滤器是否应该设置过期时间？
A：布隆过滤器终究是存在于Redis内存中的数据，如果一个用户已经很久没使用APP了，他的数据继续占用在内存中未免有点浪费，所以布隆过滤器可以考虑设置一个长的过期时间，只要用户继续活跃，就一直给布隆过滤器续期。
Q：布隆过滤器过期、准确率下降后如何重建？
A：当我们需要重建布隆过滤器时，就要考虑如何重建，重建哪些点赞记录。我们先来看看B站的一个bug，这是一个我在5年前点赞过的视频，但是我今天进入到这个视频后，它显示我没点赞过这个视频，但是当我尝试点赞时，它又显示我已经点赞过了，效果见下图


如果B站也使用了布隆过滤器作为“是否点赞过某个内容”的第一层判断的话，那么就说明我的布隆过滤器里面没有这个视频的数据，所以它认为我没有点赞过，所以没有真正的查询数据库。但是当我按下点赞按钮时，这个请求会去到数据库中尝试insert一条新的记录，这时候就发现我曾经点赞过这个视频，所以返回“已赞过”的错误提示。这就说明B站的布隆过滤器中的点赞数据并非保存完整的点赞记录数据。
回到我们的问题，重建的时候，是否需要重建完整的数据呢？看起来是不必要的，太老的数据访问频率很低，出现一小部分的不一致问题或许也能接收。所以，当触发布隆过滤器的重建时，我们只把该用户最近一年的点赞记录回写到新的布隆过滤器上即可。
Q：如果布隆过滤器命中了，请求进入到数据库中，这个过程会很慢吗？
A：不会很慢，前面数据库设计中已经提到，我为【uid, messageID, messageType】这三个字段建立了一个联合唯一索引，查询某个用户是否点赞过某个内容，它需要的条件刚好就是这三个字段的值，这就满足了【覆盖索引】的条件。
② 用户的点赞过程
每当用户尝试点赞时，点赞请求都会尝试去数据库中插入一条记录。前面在数据库设计中已经提到，我为【uid, messageID, messageType】这三个字段建立了一个联合唯一索引，这个联合唯一索引要求表中不能出现三个字段都一样的记录，否则插入的SQL语句会返回唯一索引冲突错误。
如果点赞成功了，就会尝试更新Redis中的点赞历史记录缓存，如果点赞历史记录缓存过期了，就涉及到重建点赞历史记录缓存的问题。
无论是点赞历史记录的重建，还是回溯数据库查询更老的点赞记录，它的SQL语句条件都包含了uid和messageType。由索引的最左匹配原则可知，我们建立的联合唯一索引【uid, messageID, messageType】并不符合这个SQL语句的条件，那么我们需要调整联合索引第二个字段和第三个字段的位置吗？我认为不可，因为messageType是一个区分度很小的字段，它存在的意义是标识这条记录是点赞哪个内容的，是点赞图文还是点赞评论、视频等内容，它的取值只有很少的几个。建立联合索引的一个原则是尽可能让区分度大的字段放在左边，这样能更好地提升查询效率。那么是不是这个查询的效率就会很低呢？
其实不是，得益于MySQL 5.6版本实现的【索引下推】特性，即使联合索引没有符合最左匹配原则，它在尝试回表前先在联合索引上判断其他字段的数据是否符合SQL语句的条件，符合了才会回表。所以，即使没有符合最左匹配原则这个SQL语句的查询效率也不会低。

- 评论服务
参考文献：【哔哩哔哩技术】B站评论系统架构设计
主要功能：负责对评论、评论计数的管理。
数据库设计：


主要索引有
① idx_comments_subject_id，【subjectID】字段，普通索引。
② uniq_comments_comment_id，【commentID】字段，唯一索引，评论的业务id。
③ idx_comment_contents_comment_id，【commentID】字段，CommentContent表的主键索引。
Q：为什么评论内容和评论信息要份表存储？
A：这是一种【垂直分表】设计，因为评论内容是字符串数据，而且长度可能很长，如果我们把content作为一个字段跟信息一起存的话，那么评论表的叶子节点存的评论条数就会少很多，这样MySQL一次I/O读出的数据包含的评论条数也会少很多。因为我们不是所有的查询都关心评论的内容，比如说我想查询某个视频下的所有评论信息（评论id、发布时间、评论作者、楼层号等），这是一个范围查询而且不需要content字段，如果做了垂直分表，那么一个叶子节点中包含的评论信息就会更多。
缓存设计：
① 某个内容（例如图文、视频）的评论索引缓存

使用Redis的ZSet数据类型来存储，和前面提到的索引缓存一样，这个评论的索引缓存也是为了提升查询时的分页、排序速度。
② 某个评论的楼中楼评论索引缓存

使用Redis的ZSet数据类型来存储，同样也是索引缓存，不过存的是某个根评论下的楼中楼评论索引。
③ 评论内容缓存

使用Redis的String数据类型来存储，Key是评论id，Value是json形式组成的评论信息+评论内容数据。
核心业务逻辑：
① 查询某个内容的根评论，以及根评论中可能存在的楼中楼评论，最多展示根评论的前3条楼中楼评论
Todo

- Feed服务
参考文献：Feed 流系统实战
主要功能：负责整合用户关注的创作者发布的内容。
数据库设计：

主要索引有
① idx_feeds_uid_publish_time，【uid, publishTime】字段，联合索引。
缓存设计：
① 个人Timeline

个人Timeline保存的是某个用户发布过的内容id集合，通过Redis的ZSet形式去排序。
② 关注Timeline

关注Timeline也是使用Redis的ZSet去保存，存的是某个用户他关注的创作者发表的内容id集合。
核心业务逻辑：
① 推模式的实现
当创作者发布内容时，这个内容就会尝试写进【活跃用户】的关注Timeline中。如何判断一个用户是否为活跃用户呢？因为我们的关注Timeline是存储在Redis中的，所以我们给这个表设计一个过期时间，通过订阅Kafka中来自用户服务的在线消息，在线指的是用户每次Login、RefreshToken的时候都会给Redis续期。所以当创作者发布内容时，Feed服务就异步地检查他的粉丝列表，给活跃粉丝的关注Timeline中写入创作者的内容信息
② 拉模式的实现
如果一个非活跃用户上线了，由于他的关注Timeline已经过期，所以他需要从自己的关注列表中，去关注者的个人Timeline中拉取数据，构建自己的新的关注Timeline。然后再从这个关注Timeline中读取数据。

4.消息模块
- 推送服务
Todo

5.基础模块
- 邮箱服务
主要功能：作为服务端推送的一种方式，给用户发送例如验证码等邮件。
核心业务逻辑：
① 如何发送邮件
首先注册一个网易163邮箱，在邮箱设置中开启SMTP端口，然后获取登录密钥。在golang中使用gomail库来发送邮件。
因为发送邮件是一个比较耗时的操作，如果我们同步调用gomail的接口，那么很可能会导致业务超时。所以业务通过异步的方式通知邮箱服务发送邮件，也就是邮箱服务通过订阅Kafka中来自其他业务的消息，进行异步的发邮件操作。

- 长连接服务
参考文献：【百度geek说】千万级高性能长连接Go服务架构实践 ； Go协程池(2): 如何实现协程池；万字长文：手把手教你实现一套高效的IM长连接自适应心跳保活机制
主要功能：作为服务端推送的一种方式，跟客户端保持长连接，实现服务端的主动推送功能。
核心设计：
① 分层设计
长连接服务分为两个层次，连接层和会话层。
连接层负责调用网络协议接口，处理I/O数据的读、写，组装、解析业务数据包的公共字段。连接层向会话层提供统一的数据处理接口，这样即使未来在添加其他长连接协议时，会话层的代码也不用做过多改动。
会话层负责调用连接层提供的接口实现具体的业务逻辑，维护长连接的状态机，同时与业务服务进行RPC交互。
② 网络I/O模型设计
按传统的I/O模型，我们应该给一个长连接分配两个协程，read、write各一个。但是实际上不是每一个长连接都会有频繁的下行请求，所以write协程我们应该在协程池中动态地分配。至于read协程，因为需要让它阻塞地等待来自客户端的上行数据，所以read协程是需要一直维持着的。
③ 协程池设计

④ 数据包设计
为了解决TCP粘包这个问题，我们需要在运输层上面设计应用层协议。
目前业务数据包的设计分为两部分，第一部分是公共参数（固定长度），第二部分是业务数据（json），如下

a.Length，数据包长度，四个字节是一个32位的整型，标示后面数据的长度。
b.Flag，标识位，一个字节，通过这个字节的8个位来标识一些信息，比如这个包是否是心跳包。
c.Route，路由路径，三个字节，使用ASCII编码，即3个uint8，三个字节标识这个数据包想要发给哪个服务。
d.Data，业务数据，json格式的字节数据。长连接服务不关注该部分内容，由具体的业务服务去序列化、反序列化。

⑤ 心跳设计
心跳包没有业务部分的数据，只有公共参数，而且标记位的第一个bit为1，表明这个数据包是一个心跳包。心跳包由客户端发送ping，服务端回复pong。
心跳包的超时时间设置为4分30秒，主要是为了在NAT超时之前尽可能地延长心跳包的超时时间，从而减少心跳包带来的资源消耗。运营商的NAT超时时间参考数据如下

根据网上的数据，目前市面上常见APP长连接心跳时间也是按照这个思路去设计的，如下


⑥ 数据加密设计
因为TCP连接是明文传输数据的，为了防止数据被窃取而出现的安全问题，我们需要在业务层给数据进行加密。
计算机中有各式各样的数据加密方式，例如对称加密、非对称加密、散列函数加密等。因为我们的数据是需要解密的，所以直接排除掉无法恢复的散列函数加密方案，剩下的就是对称加密和非对称加密了。
关于对称加密算法和非对称加密算法，它们的优缺点我在这里就不再赘述了，简单来说对称加密算法效率更高，非对称加密算法安全性更高。经过调查后，AES对称加密算法十分符合我们的需求，它加解密速度快，虽然是对称加密，但是它的安全性也足够高了，暴力破解的耗时是以亿年为单位的。
AES算法也有若干种加密模式可以选择，分别为ECB（电子密码本）模式、CBC（密码分组链接）模式、CTR（计算器）模式、CFB（密码反馈）模式、OFB（输出反馈）模式。除了安全程度较低的ECB模式不推荐使用外，其余几种模式都是可以使用的，这里采取了比较流行的CBC模式。
在CBC模式下，明文被切分成小段，然后与前一个或初始块的密文段进行异或运算，再与一个固定的密钥进行加密。这种模式适用于需要加密大量数据的情况，因为它可以掩盖明文结构信息，提供更好的安全性。
